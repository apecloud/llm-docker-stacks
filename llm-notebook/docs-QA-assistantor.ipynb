{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An AI question-and-answer assistant demo in KubeBlocks AIGC \n",
    "Here, we will demonstrate a demo of creating an AI docs question-and-answer assistant for the KubeBlocks user documentation within the AIGC infrastructure, all built on KubeBlocks.\n",
    "\n",
    "1. First, please make sure you have completed the following preparations:\n",
    "* A qdrant cluster created by KubeBlocks named \"my-qdrant\"\n",
    "* A private gllm cluster create by KubeBlocks named \"my-gllm\"\n",
    "using `kbcli cluster list` to check the cluster status\n",
    "\n",
    "2. load text to vector embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "from text2vec import SentenceModel\n",
    "from llama_index import LangchainEmbedding\n",
    "from llama_index.readers.file.markdown_reader import MarkdownReader\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "\n",
    "class Text2VecEmbedding(Embeddings):\n",
    "    def __init__(self):\n",
    "        self.model = SentenceModel('shibing624/text2vec-base-chinese')\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        texts = list(map(lambda x: x.replace(\"\\n\", \" \"), texts))\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return embeddings.tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        return self.model.encode(text)\n",
    "        \n",
    "embedding_model = LangchainEmbedding(Text2VecEmbedding())\n",
    "vector_size = 768\n",
    "reader = MarkdownReader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. create vector database client.\n",
    "When creating a client to connect to the vector database, we need to know the backend address of that database. You can use the kbcli cluster describe command to view the vector database information in KubeBlocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client.models import VectorParams,Distance\n",
    "\"\"\"\n",
    "`kbcli cluster describe <qdrant_cluster_name>` get the qdrant server's information \n",
    "\"\"\"\n",
    "url = \"my-qdrant-qdrant.default.svc.cluster.local\"\n",
    "port = 6333\n",
    "grpc_port = 6334\n",
    "distance = \"Cosine\"\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "                url=url,\n",
    "                port=port,\n",
    "                prefer_grpc=False,\n",
    "                https=False,\n",
    "                timeout=1000, \n",
    "            )\n",
    "client.recreate_collection(collection_name=\"demo\",vectors_config=VectorParams(\n",
    "                size=vector_size,\n",
    "                distance=Distance.COSINE,\n",
    "            ))\n",
    "\n",
    "connector = QdrantVectorStore(\n",
    "            client=client,\n",
    "            collection_name=\"demo\",\n",
    "            vectors_config=VectorParams(size=vector_size, distance=distance),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. load our user-docs\n",
    "Now, we will download KubeBlocks user docs and load it into our vector database using the previously prepared embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple, Optional\n",
    "import requests\n",
    "from llama_index.readers.file.markdown_reader import MarkdownReader\n",
    "from llama_index.schema import Document,NodeRelationship, RelatedNodeInfo\n",
    "from llama_index.data_structs.data_structs import Node\n",
    "from llama_index.vector_stores.types import NodeWithEmbedding\n",
    "\n",
    "\n",
    "class http_markdown_reader:\n",
    "    def __init__(self):\n",
    "        self.reader = MarkdownReader()\n",
    "        \n",
    "    def load_data(self, url):\n",
    "        file_name = os.path.basename(url)\n",
    "        response = requests.get(url)\n",
    "        document_content = \"\"\n",
    "        if response.status_code == 200:\n",
    "            document_content = response.text\n",
    "        else:\n",
    "            print(\"An error occurred while retrieving the document content:\", response.status_code)\n",
    "\n",
    "        tups = self.parse_tups(document_content)\n",
    "        results = []\n",
    "        nodes: List[NodeWithEmbedding] = []\n",
    "        for header, value in tups:\n",
    "            if header is None:\n",
    "                results.append(Document(text=value, metadata={}))\n",
    "            else:\n",
    "                results.append(\n",
    "                    Document(text=f\"\\n\\n{header}\\n{value}\", metadata={})\n",
    "                )\n",
    "                \n",
    "        for doc in results:\n",
    "            \n",
    "            vector = embedding_model.get_text_embedding(doc.text)\n",
    "            doc.embedding = vector\n",
    "            node = Node(\n",
    "                text=doc.text,\n",
    "                doc_id=doc.doc_id,\n",
    "            )\n",
    "            node.relationships = {\n",
    "                NodeRelationship.SOURCE: RelatedNodeInfo(\n",
    "                    node_id=node.node_id, metadata={\"source\": file_name}\n",
    "                )\n",
    "            }\n",
    "            nodes.append(NodeWithEmbedding(node=node, embedding=vector))\n",
    "        addPoints = connector.add(nodes)\n",
    "        print(\"The document has been loaded into the vector database... You can view the details through the Qdrant web UI tool\")\n",
    "\n",
    "    def parse_tups(\n",
    "            self, content: str, errors: str = \"ignore\"\n",
    "    ) -> List[Tuple[Optional[str], str]]:\n",
    "        \"\"\"Parse file into tuples.\"\"\"\n",
    "\n",
    "        content = self.reader.remove_hyperlinks(content)\n",
    "        content = self.reader.remove_images(content)\n",
    "        markdown_tups = self.reader.markdown_to_tups(content)\n",
    "        return markdown_tups\n",
    "    \n",
    "    \n",
    "reader = http_markdown_reader()\n",
    "docs = reader.load_data(\"https://raw.githubusercontent.com/apecloud/kubeblocks/main/docs/user_docs/installation/install-with-kbcli/install-kbcli.md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. query-with-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import requests\n",
    "import openai\n",
    "import os\n",
    "\n",
    "query_str = \"how to install kbcli in Windows?\"\n",
    "query_contents = client.search(collection_name=\"demo\",\n",
    "              query_vector=embedding_model.get_text_embedding(query_str),\n",
    "              with_vectors=True,\n",
    "              limit=3,\n",
    "              score_threshold=0.5,\n",
    "              search_params={'exact': False, 'hnsw_ef': 128},\n",
    "              consistency=\"majority\"\n",
    "             )\n",
    "pack_context = \"\"\n",
    "for query in query_contents:\n",
    "            payload = query.payload or {}\n",
    "            text = query.payload.get(\"text\") or json.loads(\n",
    "                payload[\"_node_content\"]\n",
    "            ).get(\"text\")\n",
    "            pack_context += text\n",
    "\n",
    "'''\n",
    "you can custom your own prompt\n",
    "'''\n",
    "prompt_template = \"\"\"Q:\n",
    "上下文信息如下:\n",
    "----------------\\n\n",
    "{context}\n",
    "\\n--------------------\\n\n",
    "\n",
    "根据提供的上下文信息,然后回答问题：{query}。\n",
    "\n",
    "请确保回答准确和详细。\n",
    "A:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "prompt_str = prompt.format(query=query_str, context=pack_context)\n",
    "# check our prompt_str\n",
    "print(prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_API = \"http://my-llm-ggml.default.svc.cluster.local:8000/v1/completions\"\n",
    "data = {\n",
    "    \"prompt\": prompt_str,\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512\n",
    "}\n",
    "response = requests.post(LLM_API, json=data)\n",
    "answer = response.json().get('choices', '')\n",
    "print(answer[0][\"text\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
